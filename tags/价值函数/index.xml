<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>价值函数 on stoner</title>
    <link>https://grayicon.github.io/tags/%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0/</link>
    <description>Recent content in 价值函数 on stoner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-Hans</language>
    <lastBuildDate>Fri, 16 Aug 2019 18:31:57 +0800</lastBuildDate>
    
	<atom:link href="https://grayicon.github.io/tags/%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>漫谈神经网络价值函数</title>
      <link>https://grayicon.github.io/blog/2019-08/%E6%BC%AB%E8%B0%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0/</link>
      <pubDate>Fri, 16 Aug 2019 18:31:57 +0800</pubDate>
      
      <guid>https://grayicon.github.io/blog/2019-08/%E6%BC%AB%E8%B0%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0/</guid>
      <description>说到神经网络的价值函数，或者是损失函数，也可以叫目标函数，随意啦，就不得不提反向传播，提到反向传播，就不得不提链式求导，梯度下降，提到梯度下降，那我就不得不抛出了上面的图片。 简单描述一下这个寻优过程，初始化一个随机值，沿着梯度的反方向以一定的步子（不要太大）不停的迭代，就能寻找到一个比较不错的局部最小值，是不是很简单？你要问我什么是梯度下降，以及梯度下降与牛顿法的区别，以及为什么是局部最小，我不告</description>
    </item>
    
  </channel>
</rss>