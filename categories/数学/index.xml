<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数学 on stoner</title>
    <link>https://grayicon.github.io/categories/%E6%95%B0%E5%AD%A6/</link>
    <description>Recent content in 数学 on stoner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-Hans</language>
    <lastBuildDate>Fri, 16 Aug 2019 18:31:57 +0800</lastBuildDate>
    
	<atom:link href="https://grayicon.github.io/categories/%E6%95%B0%E5%AD%A6/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>漫谈神经网络价值函数</title>
      <link>https://grayicon.github.io/post/%E6%BC%AB%E8%B0%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0/</link>
      <pubDate>Fri, 16 Aug 2019 18:31:57 +0800</pubDate>
      
      <guid>https://grayicon.github.io/post/%E6%BC%AB%E8%B0%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0/</guid>
      <description>本文不会包含任何数学公式，内容将引起舒适。 说到神经网络的价值函数，或者是损失函数，也可以叫目标函数，随意啦，就不得不提反向传播，提到反向传播，就不得不提链式求导，梯度下降，提到梯度下降，那我就不得不抛出了上面的图片。 简单描述一下这个寻优过程，初始化一个随机值，沿着梯度的反方向以一定的步子（不要太大）不停的迭代，就能寻找到一个比较不错的局部最小值，是不是很简单？你要问我什么是梯度下降，以及梯度下降与</description>
    </item>
    
    <item>
      <title>Keras构建复杂模型的可行性</title>
      <link>https://grayicon.github.io/post/keras%E6%9E%84%E5%BB%BA%E5%A4%8D%E6%9D%82%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7/</link>
      <pubDate>Fri, 16 Aug 2019 18:18:02 +0800</pubDate>
      
      <guid>https://grayicon.github.io/post/keras%E6%9E%84%E5%BB%BA%E5%A4%8D%E6%9D%82%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7/</guid>
      <description>keras始于易，止于简。 什么意思呢？多少人是因为对keras建模过程的友好程度而上手keras，又有多少人因为keras的高度封装造成的欠灵活性而开始累觉不爱。 这里介绍一下keras的Lambda层，希望在掌握了这个trick后，能多多少少拾回些许使用keras的信心。 步入正题，Lambda，顾名思义，和python的lambda含义是类似的，这里指的是具有某种功能的layer, keras源码里</description>
    </item>
    
  </channel>
</rss>